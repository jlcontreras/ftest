N = nrow(train)
n = nrow(x)
# 1. Compute the distance from x to each row of train
D = as.matrix(dist(rbind(train, x)))
# We are only interested in the distances between the elements of x
# and train
Dx = D[(N+1):(N+n), 1:N]
y = c()
for (i in 1:n){
# 2. Sort and identify the k closes points
ind = order(Dx[i,])
ind = ind[1:k]
# 3. Vote and return the most voted
T = table(Y[ind])
y[i] = as.numeric(names(T)[which.max(T)])
}
return(y)
}
myKNN(x, X, Y, 5)
pred = myKNN(x, X, Y, 5)
y = Y[c(1, 2, 3, 130, 131)]
pred = myKNN(x, X, Y, 5)
y == pred
sum(y == pred)/length(pred)
yhat = myKNN(x, X, Y, 5)
CCR = sum(y == yhat)/length(y)
CCR
x = X[c(1, 2, 53, 130, 131),]
y = Y[c(1, 2, 53, 130, 131)]
yhat = myKNN(x, X, Y, 5)
CCR = sum(y == yhat)/length(y)
CCR
yhat = myKNN(x, X, Y, 7)
CCR = sum(y == yhat)/length(y)
CCR
x = X[c(1, 2, 53, 70, 130, 131),]
y = Y[c(1, 2, 53, 70, 130, 131)]
CCR = c()
for (k in 1:10){
yhat = myKNN(x, X, Y, k)
# Correct Classification Rate
CCR[k] = sum(y == yhat)/length(y)
}
CCR
CCR = c()
for (k in 1:50){
yhat = myKNN(x, X, Y, k)
# Correct Classification Rate
CCR[k] = sum(y == yhat)/length(y)
}
CCR
CCR = c()
for (k in 1:100){
yhat = myKNN(x, X, Y, k)
# Correct Classification Rate
CCR[k] = sum(y == yhat)/length(y)
}
CCR
x = X[c(1, 2, 10, 53, 70, 130, 131, 150),]
y = Y[c(1, 2, 10, 53, 70, 130, 131, 150)]
CCR = c()
for (k in 1:100){
yhat = myKNN(x, X, Y, k)
# Correct Classification Rate
CCR[k] = sum(y == yhat)/length(y)
}
CCR
plot(CCR)
?plot
plot(CCR, type="l")
plot(CCR, type="l", color="blue")
plot(CCR, type="l", clr="blue")
plot(CCR, type="l", clr="#003388")
?plot
plot(CCR, type="l", col="#003388")
?sample
?sample.split
??sample.split
sample(10)
sample(10, 8)
n = nrow(iris)
sample = sample(n, 0.75*n)
train = iris[sample,]
test = iris[-sample,]
train.x = train[,1:4]
train.y = train[,5]
test.x = test[,1:4]
CCR = c()
for (k in 1:30){
yhat = myKNN(test.x, train.x, train.y, k)
# Correct Classification Rate
CCR[k] = sum(test[,5] == yhat)/length(yhat)
}
plot(CCR, type="l", col="#003388")
CCR
test[,5]
train.y = as.numeric(train[,5])
test.x = test[,1:4]
test.y = as.numeric(test[,5])
CCR = c()
for (k in 1:30){
yhat = myKNN(test.x, train.x, train.y, k)
# Correct Classification Rate
CCR[k] = sum(test[,5] == yhat)/length(yhat)
}
plot(CCR, type="l", col="#003388")
CCR = c()
for (k in 1:30){
yhat = myKNN(test.x, train.x, train.y, k)
# Correct Classification Rate
CCR[k] = sum(test.y == yhat)/length(yhat)
}
plot(CCR, type="l", col="#003388")
train
test
CCR = c()
for (k in 1:30){
yhat = myKNN(test.x, train.x, train.y, k)
# Correct Classification Rate
CCR[k] = sum(test.y == yhat)/length(test.y)
}
plot(CCR, type="l", col="#003388")
length(train)
n = nrow(iris)
sample = sample(n, 0.75*n)
train = iris[sample,]
test = iris[-sample,]
train.x = train[,1:4]
train.y = as.numeric(train[,5])
test.x = test[,1:4]
test.y = as.numeric(test[,5])
CCR = c()
for (k in 1:50){
yhat = myKNN(test.x, train.x, train.y, k)
# Correct Classification Rate
CCR[k] = sum(test.y == yhat)/length(test.y)
}
plot(CCR, type="l", col="#003388")
n = nrow(iris)
sample = sample(n, 0.75*n)
train = iris[sample,]
test = iris[-sample,]
train.x = train[,1:4]
train.y = as.numeric(train[,5])
test.x = test[,1:4]
test.y = as.numeric(test[,5])
CCR = c()
for (k in 1:50){
yhat = myKNN(test.x, train.x, train.y, k)
# Correct Classification Rate
CCR[k] = sum(test.y == yhat)/length(test.y)
}
plot(CCR, type="l", col="#003388")
n = nrow(iris)
sample = sample(n, 0.75*n)
train = iris[sample,]
test = iris[-sample,]
train.x = train[,1:4]
train.y = as.numeric(train[,5])
test.x = test[,1:4]
test.y = as.numeric(test[,5])
CCR = c()
for (k in 1:50){
yhat = myKNN(test.x, train.x, train.y, k)
# Correct Classification Rate
CCR[k] = sum(test.y == yhat)/length(test.y)
}
plot(CCR, type="l", col="#003388")
n = nrow(iris)
sample = sample(n, 0.75*n)
train = iris[sample,]
test = iris[-sample,]
train.x = train[,1:4]
train.y = as.numeric(train[,5])
test.x = test[,1:4]
test.y = as.numeric(test[,5])
CCR = c()
for (k in 1:50){
yhat = myKNN(test.x, train.x, train.y, k)
# Correct Classification Rate
CCR[k] = sum(test.y == yhat)/length(test.y)
}
plot(CCR, type="l", col="#003388")
n = nrow(iris)
sample = sample(n, 0.75*n)
train = iris[sample,]
test = iris[-sample,]
train.x = train[,1:4]
train.y = as.numeric(train[,5])
test.x = test[,1:4]
test.y = as.numeric(test[,5])
CCR = c()
for (k in 1:50){
yhat = myKNN(test.x, train.x, train.y, k)
# Correct Classification Rate
CCR[k] = sum(test.y == yhat)/length(test.y)
}
plot(CCR, type="l", col="#003388")
n = nrow(iris)
sample = sample(n, 0.75*n)
train = iris[sample,]
test = iris[-sample,]
train.x = train[,1:4]
train.y = as.numeric(train[,5])
test.x = test[,1:4]
test.y = as.numeric(test[,5])
CCR = c()
for (k in 1:50){
yhat = myKNN(test.x, train.x, train.y, k)
# Correct Classification Rate
CCR[k] = sum(test.y == yhat)/length(test.y)
}
plot(CCR, type="l", col="#003388")
n = nrow(iris)
sample = sample(n, 0.75*n)
train = iris[sample,]
test = iris[-sample,]
train.x = train[,1:4]
train.y = as.numeric(train[,5])
test.x = test[,1:4]
test.y = as.numeric(test[,5])
CCR = c()
for (k in 1:50){
yhat = myKNN(test.x, train.x, train.y, k)
# Correct Classification Rate
CCR[k] = sum(test.y == yhat)/length(test.y)
}
plot(CCR, type="l", col="#003388")
plot(CCR, type="b", col="#003388")
install.packages("e1071")
require(e1071)
?svm
svm(train.x, train.y)
train.x = train[,1:4]
train.y = train[,5]
test.x  = test[,1:4]
test.y  = test[,5]
svm(train.x, train.y)
s = svm(train.x, train.y)
CCR = c()
for (i in 1:20){
k = exp(i)
model = svm(train.x, train.y, gamma = k)
pred = predict(model, train.x)
CCR[i] = sum(pred == test.y)/length(pred)
}
plot(CCR, type = "b")
warnings()
CCR = c()
for (i in 1:20){
k = exp(i)
model = svm(train.x, train.y, gamma = k)
pred = predict(model, test.x)
CCR[i] = sum(pred == test.y)/length(pred)
}
plot(CCR, type = "b")
gamma = 0.001*exp(1:20)
plot(CCR, gamma, type = "b")
CCR = c()
for (i in 1:20){
k = 0.001 * exp(i)
model = svm(train.x, train.y, gamma = k)
pred = predict(model, test.x)
CCR[i] = sum(pred == test.y)/length(pred)
}
gamma = 0.001*exp(1:20)
plot(CCR, gamma, type = "b")
plot(gamma, CCR, type = "b")
CCR = c()
gamma = seq(1e-3, 1e3, length.out = 100)
for (k in gamma){
model = svm(train.x, train.y, gamma = k)
pred = predict(model, test.x)
CCR[i] = sum(pred == test.y)/length(pred)
}
plot(gamma, CCR, type = "b")
CCR = c()
gamma = seq(1e-3, 1e3, length.out = 100)
for (k in gamma){
model = svm(train.x, train.y, gamma = k)
pred = predict(model, test.x)
CCR[i] = sum(pred == test.y)/length(pred)
}
plot(gamma, CCR, type = "b")
length(CCR)
length(gamma)
CCR = c()
gamma = seq(1e-3, 1e3, length.out = 100)
for (k in length(gamma)){
model = svm(train.x, train.y, gamma = gamma[k])
pred = predict(model, test.x)
CCR[i] = sum(pred == test.y)/length(pred)
}
plot(gamma, CCR, type = "b")
gamma
CCR
for(k in length(gamma)){}
for(k in length(gamma)){print(k)}
for (k in 1:length(gamma)){
model = svm(train.x, train.y, gamma = gamma[k])
pred = predict(model, test.x)
CCR[i] = sum(pred == test.y)/length(pred)
}
plot(gamma, CCR, type = "b")
for (k in 1:length(gamma)){
print(k)
}
CCR
gamma
CCR = c()
gamma = seq(1e-3, 1e3, length.out = 100)
for (k in 1:length(gamma)){
model = svm(train.x, train.y, gamma = gamma[k])
pred = predict(model, test.x)
CCR[i] = sum(pred == test.y)/length(pred)
}
plot(gamma, CCR, type = "b")
gamma[3]
for (k in 1:100){
model = svm(train.x, train.y, gamma = gamma[k])
pred = predict(model, test.x)
CCR[i] = sum(pred == test.y)/length(pred)
}
plot(gamma, CCR, type = "b")
gammas = seq(1e-3, 1e3, length.out = 100)
for (k in 1:100){
model = svm(train.x, train.y, gamma = gammas[k])
pred = predict(model, test.x)
CCR[i] = sum(pred == test.y)/length(pred)
}
plot(gamma, CCR, type = "b")
plot(gammas, CCR, type = "b")
CCR = c()
gammas = seq(1e-3, 1e3, length.out = 100)
for (k in 1:100){
model = svm(train.x, train.y, gamma = gammas[k])
pred = predict(model, test.x)
CCR[k] = sum(pred == test.y)/length(pred)
}
plot(gammas, CCR, type = "b")
install.packages("elasticnet")
install.packages("elasticnet")
library(elasticnet)
df = data("diabetes")
?diabetes
model = lm(y ~ x, data=train)
data("diabetes")
n = nrow(diabetes)
sample = sample(n, 0.75*n)
train = diabetes[sample,]
test  = diabetes[-sample,]
# Linear model
model = lm(y ~ x, data=train)
summary(model)
summary(model)
plot(model)
plot(model)
yhat = predict(model, test$x)
mse = mse(yhat, test$y)
mse = sum((yhat - test$y)^2)
qqplot(test$y, yhat)
abline(0,1)
mse = sum((yhat - test$y)^2)/length(yhat)
mse
diabetes
head(test$y)
summary(test)
summary(diabetes)
dim(diabetes$x)
summary(diabetes$y)
mse
sqrt(mse)
summary(model)
train$x
train$xsex
train$x$sex
train$x["sex"]
train$x
View(train)
head(train$x)
colnames(train$x)
train$x[,"sex"]
train$x[,c("sex", "age")]
model = lm(y ~ x[,c(2,3,9)], data = train)
summary(model)
summary(model)
# Predict and evaluate results
yhat = predict(model, test$x)
qqplot(test$y, yhat)
abline(0,1)
mse = sum((yhat - test$y)^2)/length(yhat)
mse
library(MASS)
??MASS
ridge.model = lm.ridge(y ~ x, lambda=1e-3, data=train)
model.ridge = lm.ridge(y ~ x, lambda=1e-3, data=train)
plot(model.ridge)
yhat.ridge = predict(model.ridge, test$x)
yhat.ridge = test$x %*% t(t(model.ridge$coef))
model.ridge$coef
t(model.ridge$coef)
t(t(model.ridge$coef))
t(t(t(model.ridge$coef)))
t(t(t(t(model.ridge$coef))))
(t(t(model.ridge$coef))
)
mse2 = sum((yhat.ridge - test$y)^2)/length(yhat.ridge)
mse2
help(summary.lm)
library(elasticnet)
data("diabetes")
n = nrow(diabetes)
sample = sample(n, 0.75*n)
train = diabetes[sample,]
test  = diabetes[-sample,]
# Linear model
model = lm(y ~ x, data=train)
summary(model)
plot(model)
attributes(p) <- NULL
return(p)
}
lmp <- function (modelobject) {
if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
f <- summary(modelobject)$fstatistic
p <- pf(f[1],f[2],f[3],lower.tail=F)
attributes(p) <- NULL
return(p)
}
model
model1
model
lmp(model)
summary(model)
diabetes
summary(diabetes)
library(ggplot2)
library(GGally)
install.packages("GGally")
install.packages("GGally")
install.packages("GGally")
install.packages("GGally")
library(ggplot2)
library(GGally)
install.packages("GGally",
repos=c("http://rstudio.org/_packages",
"http://cran.rstudio.com"))
library(ggplot2)
install.packages("ggplot2",
repos=c("http://rstudio.org/_packages",
"http://cran.rstudio.com"))
library(ggplot2)
install.packages("scales")
install.packages("scales")
install.packages("scales")
library(ggplot2)
library(ggplot2)
library(GGally)
ggpairs(Mental)
?ggpairs
df <- read.csv("air_pollution.csv")
setwd("repos/uni/f-test/")
df <- read.csv("air_pollution.csv")
head(df)
summary(df)
model = lm(SO2 ~ popul, data=df)
summary(model)
model = lm(SO2 ~ c(popul, precip), data=df)
train = diabetes[sample,]
head(train)
?lm
model = lm(SO2 ~ c("popul", "precip"), data=df)
model = lm(df$SO2 ~ Df[,c("popul", "precip")])
model = lm(df$SO2 ~ df[,c("popul", "precip")])
model = lm(SO2 ~ popul + precip, data = df)
summary(model)
head(df)
model = lm(SO2 ~ popul + predays, data = df)
summary(model)
fitted.full <- predict(model.full, df[,c("popul","precip")])
model.full <- lm(SO2 ~ popul + precip, data = df)
fitted.full <- predict(model.full, df[,c("popul","precip")])
fitted.reduced <- predict(model.reduced, df[,c("popul","precip")])
model.full <- lm(SO2 ~ popul + precip, data = df)
# TODO: Interpret the p-values of each of the variables
summary(model.full)
model.reduced <- lm(SO2 ~ popul, data = df)
# Fit both models to our data, in order to get the error sum of squares of both models
fitted.full    <- predict(model.full,    df[,c("popul","precip")])
fitted.reduced <- predict(model.reduced, df[,c("popul","precip")])
ssef <- sum((df$SO2 - fitted.full)^2)    # Full model
sser <- sum((df$SO2 - fitted.reduced)^2) # Reduced model
model.reduced <- lm(SO2 ~ , data = df)
sser <- var(df$precip) * nrow(df)
a = c(1,3,5)
var(a)
var(a)*nrow(a)
var(a)*3
?var
sser
sser <- sum(df$precip - mean(df$precip))^2
sser
sser <- sum((df$precip - mean(df$precip))^2)
sser
k <- 2        # Number of vars used in the full model
n <- nrow(df) # Number of observations
F <- (ssef / k) / (sser / (n - k - 1))
qf(0.05, df1=k, df2=n-k-1)
df(0.05, df1=k, df2=n-k-1)
pf(0.05, df1=k, df2=n-k-1)
qf(0.05, df1=k, df2=n-k-1)
pf(0.05, df1=k, df2=n-k-1)
